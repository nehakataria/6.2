6.1.1

DRIVER

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;
public class drv {

	public static void main(String[] args) {
		Configuration conf = new Configuration();
		try {
		Job job = new Job(conf,"5.2.1");
		job.setMapperClass(mpr.class);
		job.setReducerClass(rdcr.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.setJarByClass(drv.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		Path in = new Path(args[0]);
		Path out = new Path(args[1]);
		FileInputFormat.addInputPath(job,in);
		FileOutputFormat.setOutputPath(job,out);
		job.waitForCompletion(true);
		}
		catch(Exception e)
		{
			
		}
	}

}


MAPPER

import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.io.Text;
import java.io.IOException;
public class mpr extends Mapper<LongWritable,Text,Text,Text> {
 public void map(LongWritable key,Text value, Context context) throws IOException
 {
	 try{
	 String[] s = value.toString().split("	");
	 Text k= new Text();
	 Text v= new Text();
	 if(s.length>1)
	 {
		 k.set("key");
		 v.set(s[2]);
		 context.write(k,v);
	 }
	 }
	 catch(Exception e)
	 {
		 
	 }
 }

}


REDUCER
      import org.apache.hadoop.io.LongWritable;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
public class rdcr extends Reducer<Text,Text,Text,IntWritable>
{
      public void reduce(Text key, Iterable<Text> value,Context context)throws IOException
      {
    	  Text k= new Text();
    		 IntWritable v= new IntWritable();
    		 try{
    	  int sum=0;
    	  for(Text i: value)
    	  {
    		  if((i.toString().equals("MIA"))||(i.toString().equals("MCO")))
    		  sum= sum+1;
    	  }
    	  k.set("total");
    	  v.set(sum);
    	   context.write(k,v);
    		 }
    		 catch(Exception e)
    		 {
    			 
    		 }
      }
}


6.1.2

DRIVER
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;
public class drv {

	public static void main(String[] args) {
		Configuration conf = new Configuration();
		try {
		Job job = new Job(conf,"5.2.1");
		job.setMapperClass(mpr.class);
		job.setReducerClass(rdcr.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.setJarByClass(drv.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		Path in = new Path(args[0]);
		Path out = new Path(args[1]);
		FileInputFormat.addInputPath(job,in);
		FileOutputFormat.setOutputPath(job,out);
		job.waitForCompletion(true);
		}
		catch(Exception e)
		{
			
		}
	}

}

MAPPER


MAPPER

import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.io.Text;
import java.io.IOException;
public class mpr extends Mapper<LongWritable,Text,Text,Text> {
 public void map(LongWritable key,Text value, Context context) throws IOException
 {
	 try{
	 String[] s = value.toString().split("	");
	 Text k= new Text();
	 Text v= new Text();
	 if(s.length>1)
	 {
		 k.set("key");
		 v.set(s[1]);
		 context.write(k,v);
	 }
	 }
	 catch(Exception e)
	 {
		 
	 }
 }

}


REDUCER
      import org.apache.hadoop.io.LongWritable;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
public class rdcr extends Reducer<Text,Text,Text,IntWritable>
{
      public void reduce(Text key, Iterable<Text> value,Context context)throws IOException
      {
    	  Text k= new Text();
    		 IntWritable v= new IntWritable();
    		 try{
    	  int sum=0;
    	  for(Text i: value)
    	  {
    		  if((i.toString().equals("MIA"))||(i.toString().equals("HOU")))
    		  sum= sum+1;
    	  }
    	  k.set("total");
    	  v.set(sum);
    	   context.write(k,v);
    		 }
    		 catch(Exception e)
    		 {
    			 
    		 }
      }
}


6.1.3
DRIVER

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.IntWritable;

import org.apache.hadoop.io.Text;
public class drv {

	public static void main(String[] args) {
		Configuration conf = new Configuration();
		try {
		Job job = new Job(conf,"5.2.1");
		job.setMapperClass(mpr.class);
		job.setReducerClass(rdcr.class);
		
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		job.setJarByClass(drv.class);
		
		job.setInputFormatClass(TextInputFormat.class);
		job.setOutputFormatClass(TextOutputFormat.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);
		
		Path in = new Path(args[0]);
		Path out = new Path(args[1]);
		FileInputFormat.addInputPath(job,in);
		FileOutputFormat.setOutputPath(job,out);
		job.waitForCompletion(true);
		}
		catch(Exception e)
		{
			
		}
	}

}

MAPPER

import org.apache.hadoop.conf.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.io.Text;
import java.io.IOException;
public class mpr extends Mapper<LongWritable,Text,Text,Text> {
 public void map(LongWritable key,Text value, Context context) throws IOException
 {
	 try{
	 String[] s = value.toString().split("	");
	 Text k= new Text();
	 Text v= new Text();
	 if(s.length>1)
	 {
		 if(s[3].equals("1")||s[3].equals("3")||s[3].equals("5")||s[3].equals("7"))
	 
	 {
		 k.set("key");
		 v.set(s[2]);
		 context.write(k,v);
	 }
	 }
	 }
	 catch(Exception e)
	 {
		 
	 }
 }

}

REDUCER

import org.apache.hadoop.io.LongWritable;

import java.io.IOException;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.io.Text;
public class rdcr extends Reducer<Text,Text,Text,IntWritable>
{
      public void reduce(Text key, Iterable<Text> value,Context context)throws IOException
      {
    	  Text k= new Text();
    		 IntWritable v= new IntWritable();
    		 try{
    	  int sum=0;
    	  for(Text i: value)
    	  {
    		  if((i.toString().equals("LAS"))||(i.toString().equals("LAX")))
    		  {
    		  sum= sum+1;
    		  }
    	  }
    	  k.set("total");
    	  v.set(sum);
    	   context.write(k,v);
    		 }
    		 catch(Exception e)
    		 {
    			 
    		 }
      }
}
